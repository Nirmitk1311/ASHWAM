# Ashwam Exercise A: Evidence-Grounded Extraction & Evaluation\n\nThis project implements a mini evaluation pipeline for evidence-grounded extraction from Ashwam journal entries, as per the requirements of Exercise A.\n\n## 1) Extraction Schema Design\n\nWe define a `SemanticObject` to represent extracted entities. This schema balances the need for objective evaluation with the inherent messiness and non-canonical nature of Ashwam journal data.\n\n### SemanticObject Structure:\n\n```json\n{\n  \"domain\": \"symptom\" | \"food\" | \"emotion\" | \"mind\",\n  \"evidence_span\": \"string\" (exact quote from journal),\n  \"polarity\": \"present\" | \"absent\" | \"uncertain\",\n  \"intensity_bucket\": \"low\" | \"medium\" | \"high\" | \"unknown\" (for symptom, food, mind),\n  \"arousal_bucket\": \"low\" | \"medium\" | \"high\" | \"unknown\" (for emotion),\n  \"time_bucket\": \"today\" | \"last_night\" | \"past_week\" | \"unknown\"\n}\n```\n\n### Field Constraints:\n\n*   **Constrained Fields:** `domain`, `polarity`, `intensity_bucket`, `arousal_bucket`, `time_bucket` are strictly defined enums. This ensures consistency and facilitates objective evaluation.\n*   **Free-Text Fields:** `evidence_span` is a free-text string, capturing the exact relevant phrase from the journal entry. This is crucial for evidence grounding and allows for the non-canonical nature of the journal content.\n\n### Why this schema supports:\n\n*   **Safety (no hallucinations):** The `evidence_span` field *forces* the extraction system to ground every extracted semantic object in the original text. If an entity cannot be directly evidenced in the text, it should not be extracted, or its `polarity` should be marked as `uncertain` if there\'s ambiguous textual evidence. This directly mitigates hallucination by requiring direct textual support.\n\n*   **Evaluation (objective scoring):** Constrained fields like `domain`, `polarity`, and the `_bucket` fields allow for direct, objective comparison against gold labels. The `evidence_span` enables overlap-based matching, which is essential for non-canonical labels. We can compare the extracted `evidence_span` with the gold `evidence_span` for overlap, and then check for exact matches on the constrained fields. This allows us to score without relying on fixed vocabularies for the concepts themselves.\n\n*   **Extensibility (future attributes):** The schema is flexible enough to accommodate additional attributes for `SemanticObject`s in the future. New constrained fields can be added (e.g., `severity_scale` for symptoms, `meal_type` for food) without altering the fundamental structure. Free-text fields could also be added for more nuanced observations if needed.\n\n## 2) Proposed Extraction Approach\n\nFor this exercise, we will simulate the extraction process by loading the provided `sample_predictions.jsonl` dataset. In a real-world scenario, the extraction would likely follow an **LLM + Rules-based Pipeline** approach:\n\n### Pipeline Steps:\n\n1.  **Initial LLM Extraction:** A Large Language Model would process the journal entry, identifying semantic objects (symptoms, food, emotions, mind concepts) and extracting their `domain`, `evidence_span`, `polarity`, and relevant `_bucket` values based on a meticulously crafted prompt.\n\n2.  **Post-processing and Validation (Rules-based):** A subsequent rules-based component would:\n    *   **Evidence Span Validation:** Verify that each `evidence_span` is an *exact substring* of the original journal text.\n    *   **Schema Conformance:** Ensure all extracted fields adhere to the predefined types and constrained enum values in the `SemanticObject` schema.\n    *   **Domain-Specific Bucket Assignment:** Correctly assign `intensity_bucket` for `symptom`, `food`, `mind` domains and `arousal_bucket` for `emotion`.\n\n### How Evidence Grounding is Enforced:\n\n*   **Strict Prompting:** The LLM would be explicitly instructed to provide `evidence_span` values that are direct, unaltered quotes from the journal text.\n*   **Post-Extraction Validation:** The rules-based post-processing acts as a critical safeguard, programmatically checking the validity of each `evidence_span` against the source text. Any `evidence_span` that is not a perfect substring would be flagged or the entire object potentially discarded or marked as uncertain.\n\n### How Uncertainty / Abstention is Handled Safely:\n\n*   **`polarity: \"uncertain\"`:** The LLM would be guided to use `\"uncertain\"` for `polarity` when the evidence in the journal text is ambiguous or vague.\n*   **Abstention:** The system is designed to abstain from extracting an object entirely if there is no clear and direct textual evidence to support it. This is reinforced by the `evidence_span` validation: if no valid `evidence_span` can be found, no object will be extracted, preventing hallucinations.\n\n## 3) Designed Evaluation Method\n\nThe evaluation method is designed to objectively score the extraction performance without relying on canonical labels for the extracted concepts themselves. Instead, it leverages evidence spans and constrained categorical fields.\n\n### A) How Predicted Objects are Matched to Gold Objects\n\n1.  **Primary Matching Criteria:** A predicted `SemanticObject` is considered a match for a gold `SemanticObject` if **both** of the following conditions are met:\n    *   **Domain Match:** The `domain` field of the predicted object must be identical to the `domain` field of the gold object (e.g., both are `\"symptom\"`).\n    *   **Evidence Span Overlap:** The `evidence_span` of the predicted object and the gold object must have a Jaccard similarity of \(\ge 0.5\). Jaccard similarity is calculated on the word sets of the lowercased evidence spans.\n\n2.  **One-to-One Greedy Matching:** The matching process proceeds greedily. For each journal entry, we consider all possible predicted-gold object pairs that meet the primary matching criteria. These potential matches are then sorted by their Jaccard similarity (highest first). We then iterate through this sorted list, making matches. Once a predicted object or a gold object is part of a match, it cannot be matched again. This ensures that each gold object is matched by at most one predicted object, and vice-versa.\n\n### B) What Counts as TP / FP / FN without Canonical Labels\n\n*   **True Positive (TP):** A gold object is counted as a True Positive if it is successfully matched with a predicted object based on the defined matching criteria.\n*   **False Positive (FP):** A predicted object is counted as a False Positive if it does not find any matching gold object.\n*   **False Negative (FN):** A gold object is counted as a False Negative if it does not find any matching predicted object.\n\n### C) How Scoring Works for Polarity and Bucket Attributes\n\nThese attribute scores are computed *only for True Positive (matched) object pairs*.\n\n*   **Polarity Accuracy:** For each TP pair, we compare the `polarity` values of the gold and predicted objects. If they are identical, it's considered a correct polarity prediction for that object. The polarity accuracy is the percentage of TP pairs with correct polarity.\n\n*   **Bucket Accuracy (Intensity/Arousal/Time):** Similar to polarity, for each TP pair, we compare the relevant bucket attribute. This means `intensity_bucket` for `symptom`, `food`, and `mind` domains, and `arousal_bucket` for the `emotion` domain, along with `time_bucket` for all domains. An exact string match is required for a correct bucket prediction. The bucket accuracy is the percentage of TP pairs with correct bucket attributes.\n    *   **Clearly Defined Rule for Bucket Accuracy:** An exact string comparison is used. If a bucket is `\"unknown\"` in both gold and predicted, it counts as a match. If it\'s `\"unknown\"` in one and a specific value in the other (and vice-versa), it\'s a mismatch.\n\n### D) Evidence Coverage Rate\n\nThis metric measures the proportion of predicted objects whose `evidence_span` is a valid substring within the original journal text. This is a direct measure of the system\'s ability to avoid hallucinations.\n\n*   **Calculation:**\n    \[\n    \\text{Evidence Coverage Rate} = \\frac{\\text{Number of Predicted Objects with Valid Evidence Spans}}{\\text{Total Number of Predicted Objects}}\n    \]\n    A \"valid evidence span\" means the `evidence_span` string exists verbatim in the journal entry.\n