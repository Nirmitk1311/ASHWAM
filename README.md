# Ashwam Exercise A: Evidence-Grounded Extraction & Evaluation\n\nThis project implements a mini evaluation pipeline that scores non-canonical extraction using evidence spans as the anchor, as per the requirements of Exercise A.\n\n## What was built? (End-to-end description)\n\nThis project delivers a complete mini-pipeline for evidence-grounded information extraction and evaluation from unstructured Ashwam journal entries. The core problem it solves is to derive structured, quantifiable data (like symptoms, food, emotions, and mind concepts with their polarity and intensity) from free-text human narratives, all while ensuring that every piece of extracted information is directly traceable to its source text. This is critical in domains where precise, verifiable data is needed, but the source text doesn't lend itself to rigid, pre-defined canonical labels.\n\nThe pipeline takes in a dataset of journal entries and a corresponding gold standard (human annotations), along with a set of predicted extractions (simulated for this exercise). It then meticulously compares the predicted outputs against the gold standard using a custom scoring mechanism that prioritizes exact evidence grounding and attribute consistency. Finally, it generates comprehensive performance metrics at both the overall system level and for each individual journal entry.\n\n## Key assumptions made\n\nTo scope and simplify the problem, the following key assumptions were made:\n\n*   **Extraction Simulation:** For Exercise A, the extraction process itself is simulated. Instead of implementing a real LLM-based extraction model, the system directly loads pre-provided `sample_predictions.jsonl`. This shifts the focus to the evaluation harness rather than the complexities of LLM prompting and inference.\n*   **Single, Contiguous Evidence Spans:** The `SemanticObject` schema assumes that each extracted concept corresponds to a single, contiguous `evidence_span` within the journal text. It does not account for concepts that might be implied by discontinuous phrases or require multiple, separate textual segments.\n*   **Simple Jaccard Similarity for Span Matching:** The primary matching algorithm for evidence spans relies on basic word-level Jaccard similarity. This is a simplification that works for direct overlaps but might not capture semantic equivalence in cases of paraphrasing or minor linguistic variations.\n*   **Exact Attribute Matching:** For `polarity` and `_bucket` attributes, the evaluation requires an exact string match. No partial credit or fuzzy matching is applied to these categorical fields.\n*   **English/Hinglish Language Assumption:** The provided journal entries are in English or Hinglish. The current tokenization and Jaccard similarity calculation assume word boundaries that work well for these languages.\n\n## System / pipeline breakdown\n\nThe system is composed of the following main components and stages:\n\n1.  **Data Loading (`main.py`):**\n    *   **Responsibility:** Reads and parses the input data files (`journals.jsonl`, `gold.jsonl`, `sample_predictions.jsonl`). It uses a generic `load_data_from_jsonl` function for JSONL files and a specialized `load_semantic_objects` function to convert the raw dictionaries into `SemanticObject` instances.\n    *   **Location:** `main.py`\n\n2.  **Extraction Schema (`ashwam_types.py`):**\n    *   **Responsibility:** Defines the `SemanticObject` class, which is the structured representation of extracted entities. This schema includes fields like `domain`, `evidence_span`, `polarity`, `intensity_bucket`/`arousal_bucket`, and `time_bucket` with clear constraints.\n    *   **Location:** `ashwam_types.py`\n\n3.  **Scoring Engine (`scorer.py`):**\n    *   **Responsibility:** Contains the core logic for comparing predicted `SemanticObject`s against gold `SemanticObject`s. It implements the object matching algorithm (based on domain and evidence span overlap), calculates True Positives (TPs), False Positives (FPs), and False Negatives (FNs), and computes various metrics.\n    *   **Key Functions:**\n        *   `calculate_jaccard_similarity(span1, span2)`: Computes Jaccard similarity for evidence spans.\n        *   `_match_objects(gold_objects, predicted_objects)`: Performs greedy one-to-one matching.\n        *   `score_journal(journal_text, gold_objects, predicted_objects)`: Calculates per-journal metrics.\n        *   `overall_scores(all_journal_scores)` (static method): Aggregates per-journal scores into overall system metrics.\n    *   **Location:** `scorer.py`\n\n4.  **CLI Entrypoint (`main.py`):**\n    *   **Responsibility:** Provides the command-line interface for running the entire pipeline. It parses command-line arguments (`--data`, `--out`), orchestrates the loading of data, initializes the `Scorer`, iterates through each journal to perform evaluation, and writes the final `score_summary.json` and `per_journal_scores.jsonl` output files.\n    *   **Location:** `main.py`\n\n## Determinism & safety controls\n\n### Determinism:\n\nThe entire evaluation pipeline is designed to be fully deterministic. Given the same input data (`journals.jsonl`, `gold.jsonl`, `sample_predictions.jsonl`), the system will always produce identical `score_summary.json` and `per_journal_scores.jsonl` outputs. This is ensured because:\n\n*   **Fixed Algorithms:** All matching and scoring algorithms (Jaccard similarity, greedy matching, attribute comparisons) are rule-based and have no random components.\n*   **Static Data Loading:** Data loading is consistent, always parsing JSONL files in the same order.\n*   **No LLM Variability (for Exercise A):** Since we are loading pre-generated predictions, there's no LLM inference involved in this exercise, thus eliminating a common source of variability.\n\n### Safety Controls (Guardrails against Hallucinations):\n\nSafety, specifically preventing the generation of ungrounded or hallucinated information, is a paramount concern and is addressed through:\n\n*   **Evidence-Grounded Schema:** The `SemanticObject` schema mandates an `evidence_span`, forcing any extraction system (simulated or real) to explicitly state the textual evidence for each extracted concept.\n*   **Strict Evidence Span Validation:** The `Scorer` (and conceptually, the post-processing step of a real extraction pipeline) strictly verifies that every `predicted_object.evidence_span` is an *exact, contiguous substring* of the `journal_text`. If this validation fails, the predicted object cannot contribute to True Positives and will negatively impact the `evidence_coverage_rate` (and likely become a False Positive), effectively discarding hallucinated spans.\n*   **Polarity and Abstention:** The `polarity` field includes an 'uncertain' option, allowing the system to express doubt rather than making a definitive, potentially incorrect, assertion when evidence is ambiguous. In cases of insufficient or invalid evidence, the system is designed to abstain from extraction, reinforcing the 'no hallucinations' principle.\n\n## Evaluation or monitoring strategy\n\n### Exercise A: Evaluation without Canonical Labels\n\nFor Exercise A, the evaluation strategy is specifically tailored to score non-canonical extractions by leveraging evidence grounding and attribute consistency:\n\n1.  **Object Matching:** Predicted objects are matched to gold objects based on two criteria: identical `domain` and a Jaccard similarity score of \(\\ge 0.5\) between their `evidence_span`s. A greedy, one-to-one matching approach ensures fairness.\n2.  **TP/FP/FN Calculation:** Standard metrics of True Positives (TP), False Positives (FP), and False Negatives (FN) are derived from this evidence-grounded matching process.\n3.  **Metrics Computed:**\n    *   **Object-level Precision, Recall, F1:** Standard measures of overall extraction quality.\n    *   **Polarity Accuracy:** The percentage of TP pairs where `polarity` values match exactly.\n    *   **Bucket Accuracy:** The percentage of TP pairs where `intensity_bucket` or `arousal_bucket` (depending on domain) and `time_bucket` values match exactly.\n    *   **Evidence Coverage Rate:** A crucial safety metric, measuring the percentage of predicted objects whose `evidence_span` is a valid, verbatim substring of the journal text. This directly quantifies hallucination prevention.\n\n### Future Considerations for Monitoring (Exercise C context):\n\nFor production monitoring and drift detection, one would extend this:\n\n*   **Production Data Evaluation:** Regularly run the scorer against a sample of production data that has been human-annotated. This provides a 'ground truth' performance baseline.\n*   **Prediction Drift Detection:** Monitor the distribution of extracted attributes (e.g., common domains, polarity distribution, average `intensity_bucket` values) over time. Significant shifts could indicate concept drift in the underlying journal data or a degradation in the extraction model's performance.\n*   **Evidence Coverage Rate Monitoring:** This metric is highly valuable in production. A drop in evidence coverage rate would be an immediate red flag for increasing hallucinations from the LLM.\n*   **Discrepancy Analysis:** Log and analyze cases where the confidence of the LLM is high, but the evaluation score is low, to identify areas for model improvement.\n\n## Edge cases / known limitations\n\n*   **Overlapping/Nested Concepts:** The one-to-one greedy matching might struggle with closely overlapping or truly nested semantic concepts if the Jaccard similarity leads to ambiguous matches. The current greedy approach picks the best available match, but might miss valid secondary matches.\n*   **Paraphrased Evidence Spans:** The Jaccard similarity on word sets, while robust, can penalize semantically identical but syntactically different `evidence_span`s (e.g., 'feeling tired' vs. 'experiencing tiredness'). This could lead to False Negatives.\n*   **Ambiguous Language:** Journals inherently contain ambiguity, irony, and negation. While `polarity: 'absent'` and `'uncertain'` help, complex linguistic constructions can still lead to misinterpretations by the (simulated) LLM, affecting accuracy.\n*   **Domain Overlap:** Certain concepts might genuinely straddle multiple domains (e.g., 'anxiety' could be an emotion or a symptom). The strict one-to-one domain matching will count these as mismatches if the gold and predicted domains differ.\n*   **Data Sparsity for Buckets:** For attributes like `intensity_bucket` or `arousal_bucket`, if a domain is rarely present in the data, the accuracy for that specific bucket might be unreliable due to a small number of samples.\n\n## Anything you struggled with, skipped, or found unclear?\n\n*   **Package Structure & Imports (Struggled):** Initially, setting up the Python package structure and resolving circular import dependencies between `main.py` and `scorer.py` proved challenging in the interactive environment. This required moving `SemanticObject` to its own file (`ashwam_types.py`) and carefully adjusting import paths.\n*   **Powershell vs. Bash Commands (Struggled):** The difference in shell command syntax (`&&` operator, `touch` vs. `New-Item`, `rmdir` vs. `Remove-Item -Recurse -Force`) for file operations and chaining commands led to repeated errors and required specific adaptations for the Windows PowerShell environment.\n*   **Literal String Matching in `search_replace` (Struggled):** Ensuring the `old_string` parameter in the `search_replace` tool matched *exactly* the content of the file, including all whitespace and escaped characters, was a frequent source of errors due to its strictness. This required careful `read_file` calls to verify current content.\n\n## What would you improve or extend with more time?\n\n*   **Advanced Evidence Span Matching:** Implement fuzzy string matching (e.g., using `difflib` or `fuzzywuzzy`) or semantic similarity models (e.g., Sentence-BERT embeddings) for `evidence_span` comparison to be more robust to minor textual variations without losing grounding.\n*   **Full LLM Integration with Confidence Scores:** Replace the simulated extraction with a real LLM. Incorporate confidence scores from the LLM into the `SemanticObject` schema and use them in the evaluation (e.g., weighting contributions to metrics, or setting dynamic thresholds).\n*   **Human-in-the-Loop Feedback UI:** Develop a simple web-based UI for reviewing flagged extractions (e.g., low Jaccard matches, potential hallucinations) by human annotators. This feedback loop would be invaluable for continuous model improvement and gold standard refinement.\n*   **Support for Discontinuous Evidence Spans:** Extend the `SemanticObject` schema to allow a list of character offsets or multiple `evidence_span` strings to capture complex concepts that are not contiguous in the text.\n*   **Comprehensive Unit and Integration Tests:** Build a thorough test suite for `scorer.py` and `main.py` to ensure correctness, particularly for edge cases in object matching and metric calculation, and to prevent regressions.\n*   **Detailed Error Reporting & Logging:** Enhance logging throughout the pipeline to provide more granular insights into errors, data parsing issues, and reasons for mismatches during evaluation.\n*   **Containerization (Docker):** Package the application within a Docker container to ensure consistent environments and simplify deployment, especially for production readiness.\n*   **Performance Optimization:** For very large journal datasets, optimize the matching algorithms in `scorer.py` (e.g., using spatial indexing for spans or more efficient string matching libraries) to improve evaluation speed.\n